{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Request","auditID":"413ccc8e-9bc3-449f-a3d7-883b55f87f15","stage":"ResponseComplete","requestURI":"/apis/apps/v1/namespaces/openshift-multus/deployments/multus-admission-controller?fieldManager=cluster-network-operator%2Foperconfig\u0026force=true","verb":"patch","user":{"username":"system:serviceaccount:openshift-network-operator:default","uid":"a7900175-89a4-4dfa-ac39-2f96b8d7d7c8","groups":["system:serviceaccounts","system:serviceaccounts:openshift-network-operator","system:authenticated"],"extra":{"authentication.kubernetes.io/pod-name":["network-operator-5f5d44d4c4-j8dhx"],"authentication.kubernetes.io/pod-uid":["2633ace1-49c7-4302-9239-4bb1691ee705"]}},"sourceIPs":["10.0.0.7"],"userAgent":"cluster-network-operator/v0.0.0 (linux/amd64) kubernetes/$Format","objectRef":{"resource":"deployments","namespace":"openshift-multus","name":"multus-admission-controller","apiGroup":"apps","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":200},"requestObject":{"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"kubernetes.io/description":"This deployment launches the Multus admisson controller component.\n","networkoperator.openshift.io/non-critical":"","release.openshift.io/version":"4.12.14"},"labels":{"app":"multus-admission-controller","networkoperator.openshift.io/generates-operator-status":"stand-alone"},"name":"multus-admission-controller","namespace":"openshift-multus","ownerReferences":[{"apiVersion":"operator.openshift.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"Network","name":"cluster","uid":"a425c9cf-55cd-4af4-b107-b7598496df9e"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"multus-admission-controller","namespace":"openshift-multus"}},"template":{"metadata":{"annotations":{"target.workload.openshift.io/management":"{\"effect\": \"PreferredDuringScheduling\"}"},"labels":{"app":"multus-admission-controller","component":"network","namespace":"openshift-multus","openshift.io/component":"network","type":"infra"}},"spec":{"containers":[{"command":["/bin/bash","-c","set -euo pipefail\nexec /usr/bin/webhook \\\n  -bind-address=0.0.0.0 \\\n  -port=6443 \\\n  -tls-private-key-file=/etc/webhook/tls.key \\\n  -tls-cert-file=/etc/webhook/tls.crt \\\n  -metrics-listen-address=127.0.0.1:9091 \\\n  -alsologtostderr=true \\\n  -ignore-namespaces=openshift-etcd,openshift-console,openshift-ingress-canary,openshift-apiserver,openshift-apiserver-operator,openshift-authentication,openshift-authentication-operator,openshift-cloud-controller-manager,openshift-cloud-controller-manager-operator,openshift-cloud-credential-operator,openshift-cloud-network-config-controller,openshift-cluster-csi-drivers,openshift-cluster-machine-approver,openshift-cluster-node-tuning-operator,openshift-cluster-samples-operator,openshift-cluster-storage-operator,openshift-cluster-version,openshift-config-operator,openshift-console,openshift-console-operator,openshift-controller-manager,openshift-controller-manager-operator,openshift-dns,openshift-dns-operator,openshift-etcd-operator,openshift-image-registry,openshift-ingress,openshift-ingress-operator,openshift-insights,openshift-kube-apiserver,openshift-kube-apiserver-operator,openshift-kube-controller-manager,openshift-kube-controller-manager-operator,openshift-kube-scheduler,openshift-kube-scheduler-operator,openshift-kube-storage-version-migrator,openshift-kube-storage-version-migrator-operator,openshift-logging,openshift-machine-api,openshift-machine-config-operator,openshift-marketplace,openshift-monitoring,openshift-multus,openshift-network-diagnostics,openshift-network-operator,openshift-oauth-apiserver,openshift-operator-lifecycle-manager,openshift-operators-redhat,openshift-ovn-kubernetes,openshift-route-controller-manager,openshift-service-ca-operator,openshift-user-workload-monitoring"],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:bfa5ba5eb32a6dd6d521b234ed8bf9bf585eacb77806304551cf62176bd92d6f","imagePullPolicy":"IfNotPresent","name":"multus-admission-controller","ports":[{"containerPort":9091,"name":"metrics-port"}],"resources":{"requests":{"cpu":"10m","memory":"50Mi"}},"volumeMounts":[{"mountPath":"/etc/webhook","name":"webhook-certs","readOnly":true}]},{"args":["--logtostderr","--secure-listen-address=:8443","--tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256","--upstream=http://127.0.0.1:9091/","--tls-private-key-file=/etc/webhook/tls.key","--tls-cert-file=/etc/webhook/tls.crt"],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:20bfb196dcbb88f23fb90905eba48d6116371cf5c25791b7462caf093a5e34d7","name":"kube-rbac-proxy","ports":[{"containerPort":8443,"name":"https"}],"resources":{"requests":{"cpu":"10m","memory":"20Mi"}},"terminationMessagePolicy":"FallbackToLogsOnError","volumeMounts":[{"mountPath":"/etc/webhook","name":"webhook-certs","readOnly":true}]}],"nodeSelector":{"node-role.kubernetes.io/master":""},"priorityClassName":"system-cluster-critical","restartPolicy":"Always","securityContext":{"runAsNonRoot":true,"runAsUser":65534},"serviceAccountName":"multus-ac","tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master","operator":"Exists"}],"volumes":[{"name":"webhook-certs","secret":{"secretName":"multus-admission-controller-secret"}}]}}}},"requestReceivedTimestamp":"2023-06-12T13:30:25.823875Z","stageTimestamp":"2023-06-12T13:30:25.831735Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"RBAC: allowed by ClusterRoleBinding \"default-account-cluster-network-operator\" of ClusterRole \"cluster-admin\" to ServiceAccount \"default/openshift-network-operator\""}}
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Request","auditID":"71f7e717-fecd-4205-82cb-fb804d9fa9f9","stage":"ResponseComplete","requestURI":"/apis/monitoring.coreos.com/v1/namespaces/openshift-multus/servicemonitors/monitor-multus-admission-controller?fieldManager=cluster-network-operator%2Foperconfig\u0026force=true","verb":"patch","user":{"username":"system:serviceaccount:openshift-network-operator:default","uid":"a7900175-89a4-4dfa-ac39-2f96b8d7d7c8","groups":["system:serviceaccounts","system:serviceaccounts:openshift-network-operator","system:authenticated"],"extra":{"authentication.kubernetes.io/pod-name":["network-operator-5f5d44d4c4-j8dhx"],"authentication.kubernetes.io/pod-uid":["2633ace1-49c7-4302-9239-4bb1691ee705"]}},"sourceIPs":["10.0.0.7"],"userAgent":"cluster-network-operator/v0.0.0 (linux/amd64) kubernetes/$Format","objectRef":{"resource":"servicemonitors","namespace":"openshift-multus","name":"monitor-multus-admission-controller","apiGroup":"monitoring.coreos.com","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":200},"requestObject":{"apiVersion":"monitoring.coreos.com/v1","kind":"ServiceMonitor","metadata":{"annotations":{"networkoperator.openshift.io/ignore-errors":""},"labels":{"name":"monitor-multus-admission-controller"},"name":"monitor-multus-admission-controller","namespace":"openshift-multus","ownerReferences":[{"apiVersion":"operator.openshift.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"Network","name":"cluster","uid":"a425c9cf-55cd-4af4-b107-b7598496df9e"}]},"spec":{"endpoints":[{"bearerTokenFile":"/var/run/secrets/kubernetes.io/serviceaccount/token","interval":"30s","port":"metrics","scheme":"https","tlsConfig":{"caFile":"/etc/prometheus/configmaps/serving-certs-ca-bundle/service-ca.crt","serverName":"multus-admission-controller.openshift-multus.svc"}}],"jobLabel":"app","namespaceSelector":{"matchNames":["openshift-multus"]},"selector":{"matchLabels":{"app":"multus-admission-controller"}}}},"requestReceivedTimestamp":"2023-06-12T13:30:26.023720Z","stageTimestamp":"2023-06-12T13:30:26.028569Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"RBAC: allowed by ClusterRoleBinding \"default-account-cluster-network-operator\" of ClusterRole \"cluster-admin\" to ServiceAccount \"default/openshift-network-operator\""}}
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Request","auditID":"a2c86366-9635-477a-82f1-af580b8decb1","stage":"ResponseComplete","requestURI":"/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-multus/roles/prometheus-k8s?fieldManager=cluster-network-operator%2Foperconfig\u0026force=true","verb":"patch","user":{"username":"system:serviceaccount:openshift-network-operator:default","uid":"a7900175-89a4-4dfa-ac39-2f96b8d7d7c8","groups":["system:serviceaccounts","system:serviceaccounts:openshift-network-operator","system:authenticated"],"extra":{"authentication.kubernetes.io/pod-name":["network-operator-5f5d44d4c4-j8dhx"],"authentication.kubernetes.io/pod-uid":["2633ace1-49c7-4302-9239-4bb1691ee705"]}},"sourceIPs":["10.0.0.7"],"userAgent":"cluster-network-operator/v0.0.0 (linux/amd64) kubernetes/$Format","objectRef":{"resource":"roles","namespace":"openshift-multus","name":"prometheus-k8s","apiGroup":"rbac.authorization.k8s.io","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":200},"requestObject":{"apiVersion":"rbac.authorization.k8s.io/v1","kind":"Role","metadata":{"name":"prometheus-k8s","namespace":"openshift-multus","ownerReferences":[{"apiVersion":"operator.openshift.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"Network","name":"cluster","uid":"a425c9cf-55cd-4af4-b107-b7598496df9e"}]},"rules":[{"apiGroups":[""],"resources":["services","endpoints","pods"],"verbs":["get","list","watch"]}]},"requestReceivedTimestamp":"2023-06-12T13:30:26.221542Z","stageTimestamp":"2023-06-12T13:30:26.223740Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"RBAC: allowed by ClusterRoleBinding \"default-account-cluster-network-operator\" of ClusterRole \"cluster-admin\" to ServiceAccount \"default/openshift-network-operator\""}}
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Request","auditID":"6e537d79-307a-4eea-aed2-a18eaceb0dad","stage":"ResponseComplete","requestURI":"/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-multus/rolebindings/prometheus-k8s?fieldManager=cluster-network-operator%2Foperconfig\u0026force=true","verb":"patch","user":{"username":"system:serviceaccount:openshift-network-operator:default","uid":"a7900175-89a4-4dfa-ac39-2f96b8d7d7c8","groups":["system:serviceaccounts","system:serviceaccounts:openshift-network-operator","system:authenticated"],"extra":{"authentication.kubernetes.io/pod-name":["network-operator-5f5d44d4c4-j8dhx"],"authentication.kubernetes.io/pod-uid":["2633ace1-49c7-4302-9239-4bb1691ee705"]}},"sourceIPs":["10.0.0.7"],"userAgent":"cluster-network-operator/v0.0.0 (linux/amd64) kubernetes/$Format","objectRef":{"resource":"rolebindings","namespace":"openshift-multus","name":"prometheus-k8s","apiGroup":"rbac.authorization.k8s.io","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":200},"requestObject":{"apiVersion":"rbac.authorization.k8s.io/v1","kind":"RoleBinding","metadata":{"name":"prometheus-k8s","namespace":"openshift-multus","ownerReferences":[{"apiVersion":"operator.openshift.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"Network","name":"cluster","uid":"a425c9cf-55cd-4af4-b107-b7598496df9e"}]},"roleRef":{"apiGroup":"rbac.authorization.k8s.io","kind":"Role","name":"prometheus-k8s"},"subjects":[{"kind":"ServiceAccount","name":"prometheus-k8s","namespace":"openshift-monitoring"}]},"requestReceivedTimestamp":"2023-06-12T13:30:26.421928Z","stageTimestamp":"2023-06-12T13:30:26.424510Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"RBAC: allowed by ClusterRoleBinding \"default-account-cluster-network-operator\" of ClusterRole \"cluster-admin\" to ServiceAccount \"default/openshift-network-operator\""}}
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Request","auditID":"4200fd83-c0fd-4969-9f80-4f63105eb8fd","stage":"ResponseComplete","requestURI":"/apis/monitoring.coreos.com/v1/namespaces/openshift-multus/prometheusrules/prometheus-k8s-rules?fieldManager=cluster-network-operator%2Foperconfig\u0026force=true","verb":"patch","user":{"username":"system:serviceaccount:openshift-network-operator:default","uid":"a7900175-89a4-4dfa-ac39-2f96b8d7d7c8","groups":["system:serviceaccounts","system:serviceaccounts:openshift-network-operator","system:authenticated"],"extra":{"authentication.kubernetes.io/pod-name":["network-operator-5f5d44d4c4-j8dhx"],"authentication.kubernetes.io/pod-uid":["2633ace1-49c7-4302-9239-4bb1691ee705"]}},"sourceIPs":["10.0.0.7"],"userAgent":"cluster-network-operator/v0.0.0 (linux/amd64) kubernetes/$Format","objectRef":{"resource":"prometheusrules","namespace":"openshift-multus","name":"prometheus-k8s-rules","apiGroup":"monitoring.coreos.com","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":200},"requestObject":{"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"annotations":{"networkoperator.openshift.io/ignore-errors":""},"labels":{"prometheus":"k8s","role":"alert-rules"},"name":"prometheus-k8s-rules","namespace":"openshift-multus","ownerReferences":[{"apiVersion":"operator.openshift.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"Network","name":"cluster","uid":"a425c9cf-55cd-4af4-b107-b7598496df9e"}]},"spec":{"groups":[{"name":"multus-admission-controller-monitor-service.rules","rules":[{"expr":"max  (network_attachment_definition_enabled_instance_up) by (networks)\n","record":"cluster:network_attachment_definition_enabled_instance_up:max"},{"expr":"max  (network_attachment_definition_instances) by (networks)\n","record":"cluster:network_attachment_definition_instances:max"}]}]}},"requestReceivedTimestamp":"2023-06-12T13:30:26.627107Z","stageTimestamp":"2023-06-12T13:30:26.643403Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"RBAC: allowed by ClusterRoleBinding \"default-account-cluster-network-operator\" of ClusterRole \"cluster-admin\" to ServiceAccount \"default/openshift-network-operator\""}}
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"RequestResponse","auditID":"62e66bf5-5ba8-439e-8748-77c74f2c50d5","stage":"ResponseComplete","requestURI":"/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=apiserver%3Dtrue\u0026limit=500","verb":"list","user":{"username":"kube:admin","groups":["system:cluster-admins","system:authenticated"],"extra":{"scopes.authorization.openshift.io":["user:full"]}},"sourceIPs":["10.0.0.96"],"userAgent":"oc/4.12.0 (linux/amd64) kubernetes/31aa3e8","objectRef":{"resource":"pods","namespace":"openshift-kube-apiserver","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":200},"responseObject":{"apiVersion":"v1","items":[{"metadata":{"annotations":{"kubectl.kubernetes.io/default-container":"kube-apiserver","kubernetes.io/config.hash":"0286ad675ddbee9ea70177028b221051","kubernetes.io/config.mirror":"0286ad675ddbee9ea70177028b221051","kubernetes.io/config.seen":"2023-06-08T12:17:48.203200811-04:00","kubernetes.io/config.source":"file","target.workload.openshift.io/management":"{\"effect\": \"PreferredDuringScheduling\"}"},"creationTimestamp":"2023-06-08T16:18:09Z","labels":{"apiserver":"true","app":"openshift-kube-apiserver","revision":"16"},"name":"kube-apiserver-oscar7","namespace":"openshift-kube-apiserver","ownerReferences":[{"apiVersion":"v1","controller":true,"kind":"Node","name":"oscar7","uid":"71ae09a2-f0fb-41e3-9f98-40f32b85127e"}],"resourceVersion":"8463374","uid":"d24480c1-3aea-4fa0-8b07-a4dd82d19450"},"spec":{"containers":[{"args":["LOCK=/var/log/kube-apiserver/.lock\n# We should be able to acquire the lock immediatelly. If not, it means the init container has not released it yet and kubelet or CRI-O started container prematurely.\nexec {LOCK_FD}\u003e${LOCK} \u0026\u0026 flock --verbose -w 30 \"${LOCK_FD}\" || {\n  echo \"Failed to acquire lock for kube-apiserver. Please check setup container for details. This is likely kubelet or CRI-O bug.\"\n  exit 1\n}\nif [ -f /etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt ]; then\n  echo \"Copying system trust bundle ...\"\n  cp -f /etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem\nfi\n\nexec watch-termination --termination-touch-file=/var/log/kube-apiserver/.terminating --termination-log-file=/var/log/kube-apiserver/termination.log --graceful-termination-duration=15s --kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig -- hyperkube kube-apiserver --openshift-config=/etc/kubernetes/static-pod-resources/configmaps/config/config.yaml --advertise-address=${HOST_IP}  -v=2 --permit-address-sharing\n"],"command":["/bin/bash","-ec"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}},{"name":"STATIC_POD_VERSION","value":"16"},{"name":"HOST_IP","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"status.hostIP"}}},{"name":"GOGC","value":"100"}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74ef2cd05caf0371849498e86c30b45baebad400782ab633d9e7b6476f7c00a9","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"livez","port":6443,"scheme":"HTTPS"},"initialDelaySeconds":45,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":10},"name":"kube-apiserver","ports":[{"containerPort":6443,"hostPort":6443,"protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"readyz","port":6443,"scheme":"HTTPS"},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":10},"resources":{"requests":{"cpu":"265m","memory":"1Gi"}},"securityContext":{"privileged":true},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"FallbackToLogsOnError","volumeMounts":[{"mountPath":"/etc/kubernetes/static-pod-resources","name":"resource-dir"},{"mountPath":"/etc/kubernetes/static-pod-certs","name":"cert-dir"},{"mountPath":"/var/log/kube-apiserver","name":"audit-dir"}]},{"args":["--kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig","--namespace=$(POD_NAMESPACE)","--destination-dir=/etc/kubernetes/static-pod-certs"],"command":["cluster-kube-apiserver-operator","cert-syncer"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imagePullPolicy":"IfNotPresent","name":"kube-apiserver-cert-syncer","resources":{"requests":{"cpu":"5m","memory":"50Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"FallbackToLogsOnError","volumeMounts":[{"mountPath":"/etc/kubernetes/static-pod-resources","name":"resource-dir"},{"mountPath":"/etc/kubernetes/static-pod-certs","name":"cert-dir"}]},{"args":["--kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig","--namespace=$(POD_NAMESPACE)","-v=2"],"command":["cluster-kube-apiserver-operator","cert-regeneration-controller"],"env":[{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imagePullPolicy":"IfNotPresent","name":"kube-apiserver-cert-regeneration-controller","resources":{"requests":{"cpu":"5m","memory":"50Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"FallbackToLogsOnError","volumeMounts":[{"mountPath":"/etc/kubernetes/static-pod-resources","name":"resource-dir"}]},{"args":["--insecure-port=6080","--delegate-url=https://localhost:6443/readyz"],"command":["cluster-kube-apiserver-operator","insecure-readyz"],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imagePullPolicy":"IfNotPresent","name":"kube-apiserver-insecure-readyz","ports":[{"containerPort":6080,"hostPort":6080,"protocol":"TCP"}],"resources":{"requests":{"cpu":"5m","memory":"50Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"FallbackToLogsOnError"},{"args":["--kubeconfig","/etc/kubernetes/static-pod-certs/configmaps/check-endpoints-kubeconfig/kubeconfig","--listen","0.0.0.0:17697","--namespace","$(POD_NAMESPACE)","--v","2"],"command":["cluster-kube-apiserver-operator","check-endpoints"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"healthz","port":17697,"scheme":"HTTPS"},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":10},"name":"kube-apiserver-check-endpoints","ports":[{"containerPort":17697,"hostPort":17697,"name":"check-endpoints","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"healthz","port":17697,"scheme":"HTTPS"},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":10},"resources":{"requests":{"cpu":"10m","memory":"50Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"FallbackToLogsOnError","volumeMounts":[{"mountPath":"/etc/kubernetes/static-pod-resources","name":"resource-dir"},{"mountPath":"/etc/kubernetes/static-pod-certs","name":"cert-dir"}]}],"dnsPolicy":"ClusterFirst","enableServiceLinks":true,"hostNetwork":true,"initContainers":[{"args":["echo \"Fixing audit permissions ...\"\nchmod 0700 /var/log/kube-apiserver \u0026\u0026 touch /var/log/kube-apiserver/audit.log \u0026\u0026 chmod 0600 /var/log/kube-apiserver/*\n\nLOCK=/var/log/kube-apiserver/.lock\necho \"Acquiring exclusive lock ${LOCK} ...\"\n\n# Waiting for 15s max for old kube-apiserver's watch-termination process to exit and remove the lock.\n# Two cases:\n# 1. if kubelet does not start the old and new in parallel (i.e. works as expected), the flock will always succeed without any time.\n# 2. if kubelet does overlap old and new pods for up to 130s, the flock will wait and immediate return when the old finishes.\n#\n# NOTE: We can increase 15s for a bigger expected overlap. But a higher value means less noise about the broken kubelet behaviour, i.e. we hide a bug.\n# NOTE: Do not tweak these timings without considering the livenessProbe initialDelaySeconds\nexec {LOCK_FD}\u003e${LOCK} \u0026\u0026 flock --verbose -w 15 \"${LOCK_FD}\" || {\n  echo \"$(date -Iseconds -u) kubelet did not terminate old kube-apiserver before new one\" \u003e\u003e /var/log/kube-apiserver/lock.log\n  echo -n \": WARNING: kubelet did not terminate old kube-apiserver before new one.\"\n\n  # We failed to acquire exclusive lock, which means there is old kube-apiserver running in system.\n  # Since we utilize SO_REUSEPORT, we need to make sure the old kube-apiserver stopped listening.\n  #\n  # NOTE: This is a fallback for broken kubelet, if you observe this please report a bug.\n  echo -n \"Waiting for port 6443 to be released due to likely bug in kubelet or CRI-O \"\n  while [ -n \"$(ss -Htan state listening '( sport = 6443 or sport = 6080 )')\" ]; do\n    echo -n \".\"\n    sleep 1\n    (( tries += 1 ))\n    if [[ \"${tries}\" -gt 10 ]]; then\n      echo \"Timed out waiting for port :6443 and :6080 to be released, this is likely a bug in kubelet or CRI-O\"\n      exit 1\n    fi\n  done\n  #  This is to make sure the server has terminated independently from the lock.\n  #  After the port has been freed (requests can be pending and need 60s max).\n  sleep 65\n}\n# We cannot hold the lock from the init container to the main container. We release it here. There is no risk, at this point we know we are safe.\nflock -u \"${LOCK_FD}\"\n"],"command":["/usr/bin/timeout","100","/bin/bash","-ec"],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74ef2cd05caf0371849498e86c30b45baebad400782ab633d9e7b6476f7c00a9","imagePullPolicy":"IfNotPresent","name":"setup","resources":{"requests":{"cpu":"5m","memory":"50Mi"}},"securityContext":{"privileged":true},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"FallbackToLogsOnError","volumeMounts":[{"mountPath":"/var/log/kube-apiserver","name":"audit-dir"}]}],"nodeName":"oscar7","preemptionPolicy":"PreemptLowerPriority","priority":2000001000,"priorityClassName":"system-node-critical","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":15,"tolerations":[{"operator":"Exists"}],"volumes":[{"hostPath":{"path":"/etc/kubernetes/static-pod-resources/kube-apiserver-pod-16","type":""},"name":"resource-dir"},{"hostPath":{"path":"/etc/kubernetes/static-pod-resources/kube-apiserver-certs","type":""},"name":"cert-dir"},{"hostPath":{"path":"/var/log/kube-apiserver","type":""},"name":"audit-dir"}]},"status":{"conditions":[{"lastProbeTime":null,"lastTransitionTime":"2023-06-11T17:02:46Z","status":"True","type":"Initialized"},{"lastProbeTime":null,"lastTransitionTime":"2023-06-11T17:04:08Z","status":"True","type":"Ready"},{"lastProbeTime":null,"lastTransitionTime":"2023-06-11T17:04:08Z","status":"True","type":"ContainersReady"},{"lastProbeTime":null,"lastTransitionTime":"2023-06-11T17:02:45Z","status":"True","type":"PodScheduled"}],"containerStatuses":[{"containerID":"cri-o://ed926334f90a009f5c46a7068194724435cf298a0a739a5e761912909582264d","image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74ef2cd05caf0371849498e86c30b45baebad400782ab633d9e7b6476f7c00a9","imageID":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74ef2cd05caf0371849498e86c30b45baebad400782ab633d9e7b6476f7c00a9","lastState":{},"name":"kube-apiserver","ready":true,"restartCount":1,"started":true,"state":{"running":{"startedAt":"2023-06-11T17:02:46Z"}}},{"containerID":"cri-o://6d09ac4982122fdc289058765725483fd02872d537790f201158b6b5a348a91d","image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imageID":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","lastState":{},"name":"kube-apiserver-cert-regeneration-controller","ready":true,"restartCount":1,"started":true,"state":{"running":{"startedAt":"2023-06-11T17:02:47Z"}}},{"containerID":"cri-o://ba9ac8334ff77bd6320f6ad2c924692cc4f95d85f265d5c876004bf8fb4ee763","image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imageID":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","lastState":{},"name":"kube-apiserver-cert-syncer","ready":true,"restartCount":1,"started":true,"state":{"running":{"startedAt":"2023-06-11T17:02:46Z"}}},{"containerID":"cri-o://cf214a4cea480e9e5fe7ae511d427e71bffd812eaa0e3c54df274a06f81aa666","image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imageID":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","lastState":{"terminated":{"containerID":"cri-o://1512a0e9f6e8552b357aa5f93480dae45e81595e61b6acf7842ad98bbccbd003","exitCode":255,"finishedAt":"2023-06-11T17:02:59Z","message":"W0611 17:02:47.769846       1 cmd.go:213] Using insecure, self-signed certificates\nI0611 17:02:47.770013       1 crypto.go:601] Generating new CA for check-endpoints-signer@1686502967 cert, and key in /tmp/serving-cert-2946687169/serving-signer.crt, /tmp/serving-cert-2946687169/serving-signer.key\nI0611 17:02:48.056977       1 observer_polling.go:159] Starting file observer\nW0611 17:02:58.059221       1 builder.go:230] unable to get owner reference (falling back to namespace): Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-oscar7\": net/http: TLS handshake timeout\nI0611 17:02:58.059408       1 builder.go:262] check-endpoints version 4.12.0-202304070941.p0.g2076f3d.assembly.stream-2076f3d-2076f3d0e4fea6fca54028ec7831407173ea81f5\nI0611 17:02:58.062529       1 dynamic_serving_content.go:113] \"Loaded a new cert/key pair\" name=\"serving-cert::/tmp/serving-cert-2946687169/tls.crt::/tmp/serving-cert-2946687169/tls.key\"\nW0611 17:02:59.731491       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'\nF0611 17:02:59.731696       1 cmd.go:138] error initializing delegating authentication: unable to load configmap based request-header-client-ca-file: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:serviceaccount:openshift-kube-apiserver:check-endpoints\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"\n","reason":"Error","startedAt":"2023-06-11T17:02:47Z"}},"name":"kube-apiserver-check-endpoints","ready":true,"restartCount":3,"started":true,"state":{"running":{"startedAt":"2023-06-11T17:03:00Z"}}},{"containerID":"cri-o://db9ee8d615889d431e8fc73dd092784874b9fe8dcdb3a3de0bc025483e695ad8","image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imageID":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","lastState":{},"name":"kube-apiserver-insecure-readyz","ready":true,"restartCount":1,"started":true,"state":{"running":{"startedAt":"2023-06-11T17:02:47Z"}}}],"hostIP":"10.0.0.7","initContainerStatuses":[{"containerID":"cri-o://3cd1a2120fd26b3ea744cbffb82ad758bbd6cc615bf4738b74886658f4a74e0e","image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74ef2cd05caf0371849498e86c30b45baebad400782ab633d9e7b6476f7c00a9","imageID":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74ef2cd05caf0371849498e86c30b45baebad400782ab633d9e7b6476f7c00a9","lastState":{},"name":"setup","ready":true,"restartCount":1,"state":{"terminated":{"containerID":"cri-o://3cd1a2120fd26b3ea744cbffb82ad758bbd6cc615bf4738b74886658f4a74e0e","exitCode":0,"finishedAt":"2023-06-11T17:02:46Z","reason":"Completed","startedAt":"2023-06-11T17:02:46Z"}}}],"phase":"Running","podIP":"10.0.0.7","podIPs":[{"ip":"10.0.0.7"}],"qosClass":"Burstable","startTime":"2023-06-11T17:02:45Z"}}],"kind":"PodList"},"requestReceivedTimestamp":"2023-06-12T13:30:26.660064Z","stageTimestamp":"2023-06-12T13:30:26.664333Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"RBAC: allowed by ClusterRoleBinding \"cluster-admins\" of ClusterRole \"cluster-admin\" to Group \"system:cluster-admins\""}}
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"RequestResponse","auditID":"90cec2d4-308b-46f4-b949-498ba1e3f59d","stage":"ResponseComplete","requestURI":"/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-oscar7","verb":"get","user":{"username":"kube:admin","groups":["system:cluster-admins","system:authenticated"],"extra":{"scopes.authorization.openshift.io":["user:full"]}},"sourceIPs":["10.0.0.96"],"userAgent":"oc/4.12.0 (linux/amd64) kubernetes/31aa3e8","objectRef":{"resource":"pods","namespace":"openshift-kube-apiserver","name":"kube-apiserver-oscar7","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":200},"responseObject":{"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"kubectl.kubernetes.io/default-container":"kube-apiserver","kubernetes.io/config.hash":"0286ad675ddbee9ea70177028b221051","kubernetes.io/config.mirror":"0286ad675ddbee9ea70177028b221051","kubernetes.io/config.seen":"2023-06-08T12:17:48.203200811-04:00","kubernetes.io/config.source":"file","target.workload.openshift.io/management":"{\"effect\": \"PreferredDuringScheduling\"}"},"labels":{"apiserver":"true","app":"openshift-kube-apiserver","revision":"16"},"name":"kube-apiserver-oscar7","namespace":"openshift-kube-apiserver","ownerReferences":[{"apiVersion":"v1","controller":true,"kind":"Node","name":"oscar7","uid":"71ae09a2-f0fb-41e3-9f98-40f32b85127e"}]},"spec":{"containers":[{"args":["LOCK=/var/log/kube-apiserver/.lock\n# We should be able to acquire the lock immediatelly. If not, it means the init container has not released it yet and kubelet or CRI-O started container prematurely.\nexec {LOCK_FD}\u003e${LOCK} \u0026\u0026 flock --verbose -w 30 \"${LOCK_FD}\" || {\n  echo \"Failed to acquire lock for kube-apiserver. Please check setup container for details. This is likely kubelet or CRI-O bug.\"\n  exit 1\n}\nif [ -f /etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt ]; then\n  echo \"Copying system trust bundle ...\"\n  cp -f /etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem\nfi\n\nexec watch-termination --termination-touch-file=/var/log/kube-apiserver/.terminating --termination-log-file=/var/log/kube-apiserver/termination.log --graceful-termination-duration=15s --kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig -- hyperkube kube-apiserver --openshift-config=/etc/kubernetes/static-pod-resources/configmaps/config/config.yaml --advertise-address=${HOST_IP}  -v=2 --permit-address-sharing\n"],"command":["/bin/bash","-ec"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}},{"name":"STATIC_POD_VERSION","value":"16"},{"name":"HOST_IP","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"status.hostIP"}}},{"name":"GOGC","value":"100"}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74ef2cd05caf0371849498e86c30b45baebad400782ab633d9e7b6476f7c00a9","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"livez","port":6443,"scheme":"HTTPS"},"initialDelaySeconds":45,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":10},"name":"kube-apiserver","ports":[{"containerPort":6443,"hostPort":6443,"protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"readyz","port":6443,"scheme":"HTTPS"},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":10},"resources":{"requests":{"cpu":"265m","memory":"1Gi"}},"securityContext":{"privileged":true},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"FallbackToLogsOnError","volumeMounts":[{"mountPath":"/etc/kubernetes/static-pod-resources","name":"resource-dir"},{"mountPath":"/etc/kubernetes/static-pod-certs","name":"cert-dir"},{"mountPath":"/var/log/kube-apiserver","name":"audit-dir"}]},{"args":["--kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig","--namespace=$(POD_NAMESPACE)","--destination-dir=/etc/kubernetes/static-pod-certs"],"command":["cluster-kube-apiserver-operator","cert-syncer"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imagePullPolicy":"IfNotPresent","name":"kube-apiserver-cert-syncer","resources":{"requests":{"cpu":"5m","memory":"50Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"FallbackToLogsOnError","volumeMounts":[{"mountPath":"/etc/kubernetes/static-pod-resources","name":"resource-dir"},{"mountPath":"/etc/kubernetes/static-pod-certs","name":"cert-dir"}]},{"args":["--kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig","--namespace=$(POD_NAMESPACE)","-v=2"],"command":["cluster-kube-apiserver-operator","cert-regeneration-controller"],"env":[{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imagePullPolicy":"IfNotPresent","name":"kube-apiserver-cert-regeneration-controller","resources":{"requests":{"cpu":"5m","memory":"50Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"FallbackToLogsOnError","volumeMounts":[{"mountPath":"/etc/kubernetes/static-pod-resources","name":"resource-dir"}]},{"args":["--insecure-port=6080","--delegate-url=https://localhost:6443/readyz"],"command":["cluster-kube-apiserver-operator","insecure-readyz"],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imagePullPolicy":"IfNotPresent","name":"kube-apiserver-insecure-readyz","ports":[{"containerPort":6080,"hostPort":6080,"protocol":"TCP"}],"resources":{"requests":{"cpu":"5m","memory":"50Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"FallbackToLogsOnError"},{"args":["--kubeconfig","/etc/kubernetes/static-pod-certs/configmaps/check-endpoints-kubeconfig/kubeconfig","--listen","0.0.0.0:17697","--namespace","$(POD_NAMESPACE)","--v","2"],"command":["cluster-kube-apiserver-operator","check-endpoints"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"healthz","port":17697,"scheme":"HTTPS"},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":10},"name":"kube-apiserver-check-endpoints","ports":[{"containerPort":17697,"hostPort":17697,"name":"check-endpoints","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"healthz","port":17697,"scheme":"HTTPS"},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":10},"resources":{"requests":{"cpu":"10m","memory":"50Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"FallbackToLogsOnError","volumeMounts":[{"mountPath":"/etc/kubernetes/static-pod-resources","name":"resource-dir"},{"mountPath":"/etc/kubernetes/static-pod-certs","name":"cert-dir"}]}],"dnsPolicy":"ClusterFirst","enableServiceLinks":true,"hostNetwork":true,"initContainers":[{"args":["echo \"Fixing audit permissions ...\"\nchmod 0700 /var/log/kube-apiserver \u0026\u0026 touch /var/log/kube-apiserver/audit.log \u0026\u0026 chmod 0600 /var/log/kube-apiserver/*\n\nLOCK=/var/log/kube-apiserver/.lock\necho \"Acquiring exclusive lock ${LOCK} ...\"\n\n# Waiting for 15s max for old kube-apiserver's watch-termination process to exit and remove the lock.\n# Two cases:\n# 1. if kubelet does not start the old and new in parallel (i.e. works as expected), the flock will always succeed without any time.\n# 2. if kubelet does overlap old and new pods for up to 130s, the flock will wait and immediate return when the old finishes.\n#\n# NOTE: We can increase 15s for a bigger expected overlap. But a higher value means less noise about the broken kubelet behaviour, i.e. we hide a bug.\n# NOTE: Do not tweak these timings without considering the livenessProbe initialDelaySeconds\nexec {LOCK_FD}\u003e${LOCK} \u0026\u0026 flock --verbose -w 15 \"${LOCK_FD}\" || {\n  echo \"$(date -Iseconds -u) kubelet did not terminate old kube-apiserver before new one\" \u003e\u003e /var/log/kube-apiserver/lock.log\n  echo -n \": WARNING: kubelet did not terminate old kube-apiserver before new one.\"\n\n  # We failed to acquire exclusive lock, which means there is old kube-apiserver running in system.\n  # Since we utilize SO_REUSEPORT, we need to make sure the old kube-apiserver stopped listening.\n  #\n  # NOTE: This is a fallback for broken kubelet, if you observe this please report a bug.\n  echo -n \"Waiting for port 6443 to be released due to likely bug in kubelet or CRI-O \"\n  while [ -n \"$(ss -Htan state listening '( sport = 6443 or sport = 6080 )')\" ]; do\n    echo -n \".\"\n    sleep 1\n    (( tries += 1 ))\n    if [[ \"${tries}\" -gt 10 ]]; then\n      echo \"Timed out waiting for port :6443 and :6080 to be released, this is likely a bug in kubelet or CRI-O\"\n      exit 1\n    fi\n  done\n  #  This is to make sure the server has terminated independently from the lock.\n  #  After the port has been freed (requests can be pending and need 60s max).\n  sleep 65\n}\n# We cannot hold the lock from the init container to the main container. We release it here. There is no risk, at this point we know we are safe.\nflock -u \"${LOCK_FD}\"\n"],"command":["/usr/bin/timeout","100","/bin/bash","-ec"],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74ef2cd05caf0371849498e86c30b45baebad400782ab633d9e7b6476f7c00a9","imagePullPolicy":"IfNotPresent","name":"setup","resources":{"requests":{"cpu":"5m","memory":"50Mi"}},"securityContext":{"privileged":true},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"FallbackToLogsOnError","volumeMounts":[{"mountPath":"/var/log/kube-apiserver","name":"audit-dir"}]}],"nodeName":"oscar7","preemptionPolicy":"PreemptLowerPriority","priority":2000001000,"priorityClassName":"system-node-critical","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":15,"tolerations":[{"operator":"Exists"}],"volumes":[{"hostPath":{"path":"/etc/kubernetes/static-pod-resources/kube-apiserver-pod-16","type":""},"name":"resource-dir"},{"hostPath":{"path":"/etc/kubernetes/static-pod-resources/kube-apiserver-certs","type":""},"name":"cert-dir"},{"hostPath":{"path":"/var/log/kube-apiserver","type":""},"name":"audit-dir"}]},"status":{"containerStatuses":[{"containerID":"cri-o://ed926334f90a009f5c46a7068194724435cf298a0a739a5e761912909582264d","image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74ef2cd05caf0371849498e86c30b45baebad400782ab633d9e7b6476f7c00a9","imageID":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74ef2cd05caf0371849498e86c30b45baebad400782ab633d9e7b6476f7c00a9","lastState":{},"name":"kube-apiserver","ready":true,"restartCount":1,"started":true,"state":{"running":{"startedAt":"2023-06-11T17:02:46Z"}}},{"containerID":"cri-o://6d09ac4982122fdc289058765725483fd02872d537790f201158b6b5a348a91d","image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imageID":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","lastState":{},"name":"kube-apiserver-cert-regeneration-controller","ready":true,"restartCount":1,"started":true,"state":{"running":{"startedAt":"2023-06-11T17:02:47Z"}}},{"containerID":"cri-o://ba9ac8334ff77bd6320f6ad2c924692cc4f95d85f265d5c876004bf8fb4ee763","image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imageID":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","lastState":{},"name":"kube-apiserver-cert-syncer","ready":true,"restartCount":1,"started":true,"state":{"running":{"startedAt":"2023-06-11T17:02:46Z"}}},{"containerID":"cri-o://cf214a4cea480e9e5fe7ae511d427e71bffd812eaa0e3c54df274a06f81aa666","image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imageID":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","lastState":{"terminated":{"containerID":"cri-o://1512a0e9f6e8552b357aa5f93480dae45e81595e61b6acf7842ad98bbccbd003","exitCode":255,"finishedAt":"2023-06-11T17:02:59Z","message":"W0611 17:02:47.769846       1 cmd.go:213] Using insecure, self-signed certificates\nI0611 17:02:47.770013       1 crypto.go:601] Generating new CA for check-endpoints-signer@1686502967 cert, and key in /tmp/serving-cert-2946687169/serving-signer.crt, /tmp/serving-cert-2946687169/serving-signer.key\nI0611 17:02:48.056977       1 observer_polling.go:159] Starting file observer\nW0611 17:02:58.059221       1 builder.go:230] unable to get owner reference (falling back to namespace): Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-oscar7\": net/http: TLS handshake timeout\nI0611 17:02:58.059408       1 builder.go:262] check-endpoints version 4.12.0-202304070941.p0.g2076f3d.assembly.stream-2076f3d-2076f3d0e4fea6fca54028ec7831407173ea81f5\nI0611 17:02:58.062529       1 dynamic_serving_content.go:113] \"Loaded a new cert/key pair\" name=\"serving-cert::/tmp/serving-cert-2946687169/tls.crt::/tmp/serving-cert-2946687169/tls.key\"\nW0611 17:02:59.731491       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'\nF0611 17:02:59.731696       1 cmd.go:138] error initializing delegating authentication: unable to load configmap based request-header-client-ca-file: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:serviceaccount:openshift-kube-apiserver:check-endpoints\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"\n","reason":"Error","startedAt":"2023-06-11T17:02:47Z"}},"name":"kube-apiserver-check-endpoints","ready":true,"restartCount":3,"started":true,"state":{"running":{"startedAt":"2023-06-11T17:03:00Z"}}},{"containerID":"cri-o://db9ee8d615889d431e8fc73dd092784874b9fe8dcdb3a3de0bc025483e695ad8","image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","imageID":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8ddf4c2b9ad9f7651e91a72d96e22f53ba0a30f4c0320205cc59ea32b686a30","lastState":{},"name":"kube-apiserver-insecure-readyz","ready":true,"restartCount":1,"started":true,"state":{"running":{"startedAt":"2023-06-11T17:02:47Z"}}}],"hostIP":"10.0.0.7","initContainerStatuses":[{"containerID":"cri-o://3cd1a2120fd26b3ea744cbffb82ad758bbd6cc615bf4738b74886658f4a74e0e","image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74ef2cd05caf0371849498e86c30b45baebad400782ab633d9e7b6476f7c00a9","imageID":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74ef2cd05caf0371849498e86c30b45baebad400782ab633d9e7b6476f7c00a9","lastState":{},"name":"setup","ready":true,"restartCount":1,"state":{"terminated":{"containerID":"cri-o://3cd1a2120fd26b3ea744cbffb82ad758bbd6cc615bf4738b74886658f4a74e0e","exitCode":0,"finishedAt":"2023-06-11T17:02:46Z","reason":"Completed","startedAt":"2023-06-11T17:02:46Z"}}}],"phase":"Running","podIP":"10.0.0.7","podIPs":[{"ip":"10.0.0.7"}],"qosClass":"Burstable","startTime":"2023-06-11T17:02:45Z"}},"requestReceivedTimestamp":"2023-06-12T13:30:26.727445Z","stageTimestamp":"2023-06-12T13:30:26.730396Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"RBAC: allowed by ClusterRoleBinding \"cluster-admins\" of ClusterRole \"cluster-admin\" to Group \"system:cluster-admins\""}}
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"RequestResponse","auditID":"9f18b792-8d7f-40ac-b5d4-24d6dc420946","stage":"ResponseStarted","requestURI":"/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-oscar7/exec?command=sh\u0026command=-c\u0026command=cat+%2Fvar%2Flog%2Fkube-apiserver%2Faudit.log+%7C+tail+-n+100\u0026container=kube-apiserver\u0026stderr=true\u0026stdout=true","verb":"create","user":{"username":"kube:admin","groups":["system:cluster-admins","system:authenticated"],"extra":{"scopes.authorization.openshift.io":["user:full"]}},"sourceIPs":["10.0.0.96"],"userAgent":"oc/4.12.0 (linux/amd64) kubernetes/31aa3e8","objectRef":{"resource":"pods","namespace":"openshift-kube-apiserver","name":"kube-apiserver-oscar7","apiVersion":"v1","subresource":"exec"},"responseStatus":{"metadata":{},"code":101},"requestReceivedTimestamp":"2023-06-12T13:30:26.739033Z","stageTimestamp":"2023-06-12T13:30:26.771410Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"RBAC: allowed by ClusterRoleBinding \"cluster-admins\" of ClusterRole \"cluster-admin\" to Group \"system:cluster-admins\""}}
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Request","auditID":"28ba8447-376e-4bcf-afef-adbf4dbddcc3","stage":"ResponseComplete","requestURI":"/api/v1/namespaces/openshift-ovn-kubernetes?fieldManager=cluster-network-operator%2Foperconfig\u0026force=true","verb":"patch","user":{"username":"system:serviceaccount:openshift-network-operator:default","uid":"a7900175-89a4-4dfa-ac39-2f96b8d7d7c8","groups":["system:serviceaccounts","system:serviceaccounts:openshift-network-operator","system:authenticated"],"extra":{"authentication.kubernetes.io/pod-name":["network-operator-5f5d44d4c4-j8dhx"],"authentication.kubernetes.io/pod-uid":["2633ace1-49c7-4302-9239-4bb1691ee705"]}},"sourceIPs":["10.0.0.7"],"userAgent":"cluster-network-operator/v0.0.0 (linux/amd64) kubernetes/$Format","objectRef":{"resource":"namespaces","namespace":"openshift-ovn-kubernetes","name":"openshift-ovn-kubernetes","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":200},"requestObject":{"apiVersion":"v1","kind":"Namespace","metadata":{"annotations":{"openshift.io/description":"OVN Kubernetes components","openshift.io/node-selector":"","workload.openshift.io/allowed":"management"},"labels":{"openshift.io/cluster-monitoring":"true","openshift.io/run-level":"0","pod-security.kubernetes.io/audit":"privileged","pod-security.kubernetes.io/enforce":"privileged","pod-security.kubernetes.io/warn":"privileged"},"name":"openshift-ovn-kubernetes","ownerReferences":[{"apiVersion":"operator.openshift.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"Network","name":"cluster","uid":"a425c9cf-55cd-4af4-b107-b7598496df9e"}]}},"requestReceivedTimestamp":"2023-06-12T13:30:26.821749Z","stageTimestamp":"2023-06-12T13:30:26.825241Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"RBAC: allowed by ClusterRoleBinding \"default-account-cluster-network-operator\" of ClusterRole \"cluster-admin\" to ServiceAccount \"default/openshift-network-operator\""}}
